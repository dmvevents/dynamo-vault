# Dockerfile.v36-dynamo-efa-rc5-cuda13-devel
#
# PURPOSE: Build from original Dynamo EFA base with TRT-LLM rc5 + CUDA devel toolkit
#
# v36 FIX: Use CUDA devel image instead of runtime to include nvcc compiler
# FlashInfer JIT compilation requires nvcc which is only in the devel image
#
# Build order:
# 1. Dockerfile.efa (dynamo:dev-base-efa) - Built Dec 2
# 2. Dockerfile.dynamo-trtllm-efa (with rc5 instead of rc4)
#
# Build:
#   docker build -f Dockerfile.v36-dynamo-efa-rc5-cuda13-devel \
#     -t dynamo-trtllm:h100-v36-dynamo-efa-rc5-cuda13-devel .

##################################
########## Build Arguments #######
##################################

# Base images - use our original Dynamo EFA base
ARG DYNAMO_BASE_IMAGE="dynamo:dev-base-efa"

# PyTorch base for TRT-LLM dependencies
ARG PYTORCH_BASE_IMAGE="nvcr.io/nvidia/pytorch"
ARG PYTORCH_BASE_IMAGE_TAG="25.06-py3"

# Runtime image - CUDA 13 DEVEL for FlashInfer JIT (includes nvcc)
# v36 FIX: Changed from runtime to devel image
ARG RUNTIME_IMAGE="nvcr.io/nvidia/cuda"
ARG RUNTIME_IMAGE_TAG="13.0.0-devel-ubuntu24.04"

# Architecture
ARG ARCH=amd64
ARG ARCH_ALT=x86_64

# Python
ARG PYTHON_VERSION="3.12"

# TRT-LLM rc5 - THIS IS THE KEY CHANGE
ARG TENSORRTLLM_PIP_WHEEL="tensorrt-llm==1.2.0rc5"
ARG TENSORRTLLM_INDEX_URL="https://pypi.nvidia.com/"
ARG GITHUB_TRTLLM_COMMIT="v1.2.0rc5"

# Triton version
ARG TRITON_VERSION="3.3.1"

# Reference images
FROM ${DYNAMO_BASE_IMAGE} AS dynamo_base
FROM ${PYTORCH_BASE_IMAGE}:${PYTORCH_BASE_IMAGE_TAG} AS pytorch_base

########################################################
########## Framework Build Stage #######################
########################################################
#
# PURPOSE: Build TensorRT-LLM rc5 with PyTorch dependencies

FROM ${DYNAMO_BASE_IMAGE} AS framework

ARG ARCH_ALT
ARG PYTHON_VERSION
ARG TENSORRTLLM_PIP_WHEEL
ARG TENSORRTLLM_INDEX_URL
ARG GITHUB_TRTLLM_COMMIT
ARG TRITON_VERSION

# Environment setup
ENV NIXL_PREFIX=/opt/nvidia/nvda_nixl
ENV NIXL_LIB_DIR=${NIXL_PREFIX}/lib/${ARCH_ALT}-linux-gnu
ENV NIXL_PLUGIN_DIR=${NIXL_LIB_DIR}/plugins
ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"

# Install dependencies for TensorRT-LLM
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python${PYTHON_VERSION}-dev \
        python3-pip \
        curl \
        git \
        git-lfs \
        ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

# PyTorch versions from NGC 25.06
ARG TORCH_VER=2.7.0a0
ARG TORCH_TENSORRT_VER=2.7.0a0
ARG TORCHVISION_VER=0.22.0a0
ARG JINJA2_VER=3.1.6
ARG SYMPY_VER=1.14.0
ARG FLASH_ATTN_VER=2.7.4.post1

# Copy PyTorch and dependencies from NGC PyTorch image
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchgen ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchgen
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchvision
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision.libs ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchvision.libs
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/functorch ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/functorch
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/jinja2 ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/jinja2
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/jinja2-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/sympy ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/sympy
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/sympy-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/flash_attn ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/flash_attn
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/flash_attn-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/flash_attn_2_cuda.cpython-*-linux-gnu.so ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch_tensorrt ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch_tensorrt
COPY --from=pytorch_base /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch_tensorrt-*.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/

# Clear any existing pip constraints
RUN [ -f /etc/pip/constraint.txt ] && : > /etc/pip/constraint.txt || true && \
    rm -f /etc/apt/sources.list.d/cuda*.list 2>/dev/null || true && \
    rm -f /usr/share/keyrings/cuda-archive-keyring.gpg 2>/dev/null || true

# Install cuda-python (required for TRT-LLM)
RUN --mount=type=cache,target=/root/.cache/uv \
    echo "Installing cuda-python (required for TRT-LLM)..." && \
    uv pip install --no-cache "cuda-python>=12.0,<13.0" && \
    echo "cuda-python installed"

# Install TensorRT-LLM rc5
# First install pip and etcd3 URL dependency separately (uv doesn't handle URL deps in wheel metadata)
RUN uv pip install --no-cache pip && \
    uv pip install --no-cache "etcd3 @ git+https://github.com/kragniz/python-etcd3.git@e58a899579ba416449c4e225b61f039457c8072a"

RUN echo "Installing TRT-LLM from PyPI: ${TENSORRTLLM_PIP_WHEEL}" && \
    # Extract version for install_tensorrt.sh download
    TRTLLM_VERSION=$(echo "${TENSORRTLLM_PIP_WHEEL}" | sed -E 's/.*==([0-9a-zA-Z.+-]+).*/\1/') && \
    echo "TRT-LLM version: ${TRTLLM_VERSION}" && \
    # Download TensorRT installer
    (curl -fsSL --retry 5 --retry-delay 10 --max-time 1800 \
        -o /tmp/install_tensorrt.sh \
        "https://github.com/NVIDIA/TensorRT-LLM/raw/v${TRTLLM_VERSION}/docker/common/install_tensorrt.sh" || \
     curl -fsSL --retry 5 --retry-delay 10 --max-time 1800 \
        -o /tmp/install_tensorrt.sh \
        "https://github.com/NVIDIA/TensorRT-LLM/raw/${GITHUB_TRTLLM_COMMIT}/docker/common/install_tensorrt.sh") && \
    sed -i 's/pip3 install/uv pip install/g' /tmp/install_tensorrt.sh && \
    bash /tmp/install_tensorrt.sh && \
    echo "TensorRT installed" && \
    # Install TRT-LLM with python -m pip in venv (uv doesn't handle URL deps in wheel metadata)
    # Don't force triton version - let pip resolve dependencies
    ${VIRTUAL_ENV}/bin/python -m pip install --no-cache-dir \
        --extra-index-url "${TENSORRTLLM_INDEX_URL}" \
        "${TENSORRTLLM_PIP_WHEEL}" && \
    echo "TensorRT-LLM installed"

# Downgrade ONNX for compatibility
RUN uv pip install --no-cache 'onnx==1.16.0'

# Verify installation
RUN python -c "import torch; print(f'PyTorch: {torch.__version__}')" && \
    python -c "import tensorrt; print(f'TensorRT: {tensorrt.__version__}')" && \
    echo "Framework dependencies installed"

########################################################
########## Runtime Stage ###############################
########################################################

FROM ${RUNTIME_IMAGE}:${RUNTIME_IMAGE_TAG} AS runtime

ARG ARCH_ALT
ARG PYTHON_VERSION

# Environment setup
ENV DYNAMO_HOME=/opt/dynamo
ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"
ENV NIXL_PREFIX=/opt/nvidia/nvda_nixl
ENV NIXL_LIB_DIR=${NIXL_PREFIX}/lib/${ARCH_ALT}-linux-gnu
ENV NIXL_PLUGIN_DIR=${NIXL_LIB_DIR}/plugins

WORKDIR /workspace

# Install runtime dependencies
# v36: devel image has more packages pre-installed, but we still need some
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python${PYTHON_VERSION} \
        python${PYTHON_VERSION}-dev \
        jq \
        git \
        git-lfs \
        curl \
        libibverbs1 \
        rdma-core \
        ibverbs-utils \
        libibumad3 \
        libnuma1 \
        librdmacm1 \
        ibverbs-providers \
        libzmq5 \
        ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy NATS and etcd from base
COPY --from=dynamo_base /usr/bin/nats-server /usr/bin/nats-server
COPY --from=dynamo_base /usr/local/bin/etcd/ /usr/local/bin/etcd/

# Remove HPC-X from pytorch_base to avoid MPI conflicts
RUN rm -rf /opt/hpcx

# Copy TensorRT libraries from dynamo_base (the ACTUAL location!)
# v31 BUG FIX: TensorRT libs are in /usr/lib/x86_64-linux-gnu/, NOT /usr/local/tensorrt
# The dynamo_base image has TensorRT 10.13.3 pre-installed at this location
COPY --from=dynamo_base /usr/lib/x86_64-linux-gnu/libnvinfer* /usr/lib/x86_64-linux-gnu/
COPY --from=dynamo_base /usr/lib/x86_64-linux-gnu/libnvonnxparser* /usr/lib/x86_64-linux-gnu/

# Also copy /usr/local/tensorrt if it exists (Python bindings, headers, etc.)
COPY --from=framework /usr/local/tensorrt /usr/local/tensorrt

# Copy cuSPARSELt
COPY --from=pytorch_base /usr/local/cuda/lib64/libcusparseLt* /usr/local/cuda/lib64/

# Copy libgomp
COPY --from=framework /usr/lib/${ARCH_ALT}-linux-gnu/libgomp.so* /usr/lib/${ARCH_ALT}-linux-gnu/

# Copy uv
COPY --from=framework /bin/uv /bin/uvx /bin/

# Create dynamo user
RUN userdel -r ubuntu 2>/dev/null || true && \
    useradd -m -s /bin/bash -g 0 dynamo && \
    mkdir -p /home/dynamo/.cache /opt/dynamo && \
    chown -R dynamo: /workspace /home/dynamo /opt/dynamo && \
    chmod -R g+w /workspace /home/dynamo/.cache /opt/dynamo

USER dynamo
ENV HOME=/home/dynamo

# Copy EFA components from Dynamo base
# 1. UCX (base transport)
COPY --chown=dynamo: --from=dynamo_base /usr/local/ucx /usr/local/ucx

# 2. libfabric (EFA provider)
COPY --chown=dynamo: --from=dynamo_base /usr/local/lib/libfabric* /usr/local/lib/

# 3. AWS EFA userspace
COPY --chown=dynamo: --from=dynamo_base /opt/amazon /opt/amazon

# 4. GDRCopy
COPY --chown=dynamo: --from=dynamo_base /usr/local/lib/libgdrapi* /usr/local/lib/

# 5. System libraries needed by TensorRT-LLM bindings
COPY --from=dynamo_base /usr/lib/x86_64-linux-gnu/libhwloc.so* /usr/lib/x86_64-linux-gnu/
COPY --from=dynamo_base /usr/lib/x86_64-linux-gnu/libevent*.so* /usr/lib/x86_64-linux-gnu/

# 6. NCCL
COPY --from=dynamo_base /usr/local/lib/libnccl.so* /usr/local/lib/
COPY --from=dynamo_base /usr/local/include/nccl*.h /usr/local/include/

# 7. AWS OFI NCCL plugin
COPY --from=dynamo_base /usr/local/lib/libnccl-net* /usr/local/lib/
COPY --from=dynamo_base /usr/local/lib/libnccl-ofi-tuner* /usr/local/lib/

# 8. NIXL (this contains LIBFABRIC plugin)
COPY --chown=dynamo: --from=dynamo_base ${NIXL_PREFIX} ${NIXL_PREFIX}

# 9. Validation tools
COPY --from=dynamo_base /opt/nccl-tests /opt/nccl-tests
COPY --from=dynamo_base /usr/local/bin/nixlbench /usr/local/bin/nixlbench

# Refresh library cache
USER root
RUN ldconfig
USER dynamo

# Library Path Configuration
ENV TENSORRT_LIB_DIR=/usr/local/tensorrt/targets/${ARCH_ALT}-linux-gnu/lib

ENV PATH="\
/opt/amazon/openmpi/bin:\
/opt/amazon/efa/bin:\
/usr/local/ucx/bin:\
/usr/local/bin/etcd:\
/usr/local/cuda/bin:\
${VIRTUAL_ENV}/bin:\
${PATH}"

ENV LD_LIBRARY_PATH="\
/opt/amazon/openmpi/lib:\
/opt/amazon/efa/lib64:\
/opt/amazon/efa/lib:\
/usr/local/ucx/lib:\
/usr/local/ucx/lib/ucx:\
${NIXL_LIB_DIR}:\
${NIXL_PLUGIN_DIR}:\
${TENSORRT_LIB_DIR}:\
/usr/local/lib:\
/usr/local/cuda/lib64:\
${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch/lib:\
${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch_tensorrt/lib:\
${LD_LIBRARY_PATH}"

ENV MPI_HOME=/opt/amazon/openmpi
ENV OPAL_PREFIX=
ENV HPCX_VERSION=
ENV OMPI_MCA_plm_rsh_agent=/bin/false

# AWS EFA environment
ENV FI_EFA_FORK_SAFE=1
ENV FI_LOG_LEVEL=warn
ENV FI_PROVIDER=efa

# UCX environment
ENV UCX_TLS=self,sm
ENV UCX_NET_DEVICES=all

# TRT-LLM specific settings
ENV TRTLLM_ENABLE_PDL=1

# Copy virtual environment with TRT-LLM rc5
COPY --chown=dynamo: --from=framework ${VIRTUAL_ENV} ${VIRTUAL_ENV}

# Install Dynamo and NIXL wheels
COPY --chown=dynamo: --from=dynamo_base /opt/dynamo/wheelhouse/ /opt/dynamo/wheelhouse/

# Install Dynamo from PyPI and NIXL from wheelhouse
RUN --mount=type=cache,target=/home/dynamo/.cache/uv,uid=1000 \
    uv pip install --no-cache ai-dynamo && \
    uv pip install --no-cache /opt/dynamo/wheelhouse/nixl/*.whl

# Verify installation
RUN python -c "import dynamo; print('Dynamo orchestration')" && \
    python -c "import nixl; print(f'NIXL {nixl.__version__}')" 2>/dev/null || true

# Fix BuildConfig model_fields API incompatibility
RUN sed -i '44s/.*/        self.max_batch_size: int = 2048/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '45s/.*/        self.max_num_tokens: int = 8192/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '46s/.*/        self.max_seq_len: int = None/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '47s/.*/        self.max_beam_width: int = 1/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '177s/.*/        default=2048,/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '183s/.*/        default=8192,/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '189s/.*/        default=None,/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py && \
    sed -i '196s/.*/        default=1,/' \
        /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/utils/trtllm_utils.py

# Patch NIXL initialization (graceful fallback)
RUN python3 <<'NIXL_FIX'
import re
main_py = "/opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/main.py"
with open(main_py, 'r') as f:
    content = f.read()

original = r'(\s+)connector = None\n\s+logging\.info\("Initializing NIXL Connect\."\)\n\s+connector = nixl_connect\.Connector\(\)\n\s+await connector\.initialize\(\)'
replacement = r'''\1connector = None
\1# Try to initialize NIXL with graceful fallback
\1try:
\1    if config.disaggregation_mode.name != 'AGGREGATED':
\1        logging.info("Attempting NIXL Connect initialization...")
\1        connector = nixl_connect.Connector()
\1        await connector.initialize()
\1        logging.info("NIXL Connect initialized successfully")
\1    else:
\1        logging.info("Skipping NIXL initialization (aggregated mode)")
\1except Exception as e:
\1    logging.warning(f"NIXL initialization failed: {e}")
\1    logging.warning("Continuing without NIXL - will use TensorRT-LLM native KV cache transfer")
\1    connector = None'''
content = re.sub(original, replacement, content, flags=re.MULTILINE)
with open(main_py, 'w') as f:
    f.write(content)
print("NIXL optional initialization patch applied")
NIXL_FIX

# ============================================================================
# ONLY REMAINING PATCH: nixl_connect NIXL_BACKEND env var support
# This is the one patch that is still needed even with pure rc5
# ============================================================================
RUN python3 <<'NIXL_BACKEND_PATCH'
import re
nixl_connect_file = "/opt/dynamo/venv/lib/python3.12/site-packages/dynamo/nixl_connect/__init__.py"
with open(nixl_connect_file, 'r') as f:
    content = f.read()

# Find and replace the nixl_agent initialization
old_code = "self._nixl = nixl_api.nixl_agent(self._worker_id)"
new_code = '''# LIBFABRIC PATCH: Read NIXL_BACKEND env var and configure agent
        import os as _nixl_os
        _nixl_backend = _nixl_os.environ.get('NIXL_BACKEND', 'UCX')
        _backends = [_nixl_backend] if _nixl_backend != 'UCX' else ['UCX']
        logger.info(f"NIXL Connect: Using backend(s): {_backends}")
        _nixl_config = nixl_api.nixl_agent_config(backends=_backends)
        self._nixl = nixl_api.nixl_agent(self._worker_id, _nixl_config)'''

if old_code in content:
    content = content.replace(old_code, new_code)
    with open(nixl_connect_file, 'w') as f:
        f.write(content)
    print("nixl_connect NIXL_BACKEND patch applied")
else:
    print("WARNING: Could not find target code to patch in nixl_connect")
NIXL_BACKEND_PATCH

# Verify nixl_connect patch
RUN grep -q "LIBFABRIC PATCH" /opt/dynamo/venv/lib/python3.12/site-packages/dynamo/nixl_connect/__init__.py && \
    echo "=== NIXL_CONNECT PATCH VERIFIED ===" || \
    echo "=== WARNING: Patch not found ==="

# v36: Add build_config removal patch for TRT-LLM rc5 PyTorch backend compatibility
# This is baked into the image so runtime YAML doesn't need complex Python patches
RUN python3 <<'BUILD_CONFIG_PATCH'
import re
main_py = "/opt/dynamo/venv/lib/python3.12/site-packages/dynamo/trtllm/main.py"
with open(main_py, 'r') as f:
    content = f.read()

# Check if patch is already applied
if 'Removing build_config for PyTorch backend' in content:
    print("build_config patch already applied")
else:
    # Find the line that logs TensorRT-LLM engine args and insert our fix before it
    fix_code = '''
    # FIX for TRT-LLM 1.2.0rc5: Remove build_config when using PyTorch backend
    # TRT-LLM rc5 has stricter validation that rejects build_config with PyTorch backend
    if arg_map.get("backend") == "pytorch" and "build_config" in arg_map:
        logging.info("Removing build_config for PyTorch backend (TRT-LLM rc5 compatibility)")
        del arg_map["build_config"]

'''
    # Insert the fix before the logging line
    old_line = '    logging.info(f"TensorRT-LLM engine args: {arg_map}")'
    new_content = content.replace(old_line, fix_code + old_line)

    if new_content != content:
        with open(main_py, 'w') as f:
            f.write(new_content)
        print("build_config patch applied for TRT-LLM rc5 PyTorch backend compatibility")
    else:
        print("WARNING: Could not find target line to patch for build_config fix")
BUILD_CONFIG_PATCH

# v36: Remove duplicate flex_attention files to prevent Triton template conflict
# PyTorch 2.9 + Triton 3.5 have these files in multiple locations causing assertion error
RUN rm -f /opt/dynamo/venv/lib/python3.12/site-packages/torch/_inductor/kernel/flex_attention.py && \
    rm -f /opt/dynamo/venv/lib/python3.12/site-packages/torch/_inductor/kernel/flex_decoding.py && \
    rm -f /opt/dynamo/venv/lib/python3.12/site-packages/torch/_inductor/kernel/__pycache__/flex_attention*.pyc && \
    rm -f /opt/dynamo/venv/lib/python3.12/site-packages/torch/_inductor/kernel/__pycache__/flex_decoding*.pyc && \
    echo "=== flex_attention files removed ==="

# Setup bash environment
USER root
RUN echo 'source /opt/dynamo/venv/bin/activate' >> /etc/bash.bashrc && \
    echo '# Dynamo TensorRT-LLM Runtime (rc5 + LIBFABRIC + CUDA devel)' >> /etc/bash.bashrc

USER dynamo

# Set default environment for LIBFABRIC
ENV NIXL_BACKEND=LIBFABRIC
ENV FI_PROVIDER=efa
ENV FI_EFA_USE_DEVICE_RDMA=1

# Verify LIBFABRIC plugin exists
RUN ls -la ${NIXL_PLUGIN_DIR}/libplugin_LIBFABRIC.so 2>/dev/null && \
    echo "=== LIBFABRIC plugin verified ===" || \
    echo "=== WARNING: LIBFABRIC plugin not found ==="

# v36: Verify nvcc is available for FlashInfer JIT
RUN which nvcc && nvcc --version && \
    echo "=== nvcc verified for FlashInfer JIT ===" || \
    echo "=== WARNING: nvcc not found ==="

# Labels
LABEL version="h100-v36-dynamo-efa-rc5-cuda13-devel"
LABEL description="Dynamo EFA base + TRT-LLM rc5 + CUDA 13 devel + all fixes baked in"
LABEL fixes="v33 flex_attention fix + v34 build_config fix + v36 nvcc for FlashInfer JIT"

ENTRYPOINT ["/bin/bash", "-c"]
CMD ["bash"]
